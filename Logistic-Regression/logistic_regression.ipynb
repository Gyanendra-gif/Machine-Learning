{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-03-17 08:48:27,709 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Demo').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"file:///home/hdoop/python_operation/ML_Operation/customer_churn.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|       Onboard_date|            Location|             Company|              Churn|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          900|              900|              900|               900|              900|               900|                900|                 900|                 900|                900|\n",
      "|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|               null|                null|                null|0.16666666666666666|\n",
      "| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|               null|                null|                null| 0.3728852122772358|\n",
      "|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|2006-01-02 04:16:13|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n",
      "|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|2016-12-28 04:07:38|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assesmbler = VectorAssembler(inputCols=['Age',\n",
    "'Total_Purchase',\n",
    "'Account_Manager',\n",
    "'Years',\n",
    "'Num_Sites'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assesmbler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = output.select(\"features\", \"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|churn|\n",
      "+--------------------+-----+\n",
      "|[42.0,11066.8,0.0...|    1|\n",
      "|[41.0,11916.22,0....|    1|\n",
      "|[38.0,12884.75,0....|    1|\n",
      "|[42.0,8010.76,0.0...|    1|\n",
      "|[37.0,9191.58,0.0...|    1|\n",
      "|[48.0,10356.02,0....|    1|\n",
      "|[44.0,11331.58,1....|    1|\n",
      "|[32.0,9885.12,1.0...|    1|\n",
      "|[43.0,14062.6,1.0...|    1|\n",
      "|[40.0,8066.94,1.0...|    1|\n",
      "|[30.0,11575.37,1....|    1|\n",
      "|[45.0,8771.02,1.0...|    1|\n",
      "|[45.0,8988.67,1.0...|    1|\n",
      "|[40.0,8283.32,1.0...|    1|\n",
      "|[41.0,6569.87,1.0...|    1|\n",
      "|[38.0,10494.82,1....|    1|\n",
      "|[45.0,8213.41,1.0...|    1|\n",
      "|[43.0,11226.88,0....|    1|\n",
      "|[53.0,5515.09,0.0...|    1|\n",
      "|[46.0,8046.4,1.0,...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_final.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 09:00:29,629 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "2022-03-17 09:00:29,644 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    }
   ],
   "source": [
    "lrm = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm_summary = lrm.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdoop/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[22.0,11254.38,1....|  0.0|[4.55979963242689...|[0.98964420919358...|       0.0|\n",
      "|[25.0,9672.03,0.0...|  0.0|[4.67536757133124...|[0.99076400091638...|       0.0|\n",
      "|[26.0,8939.61,0.0...|  0.0|[6.28230440567530...|[0.99813439848473...|       0.0|\n",
      "|[27.0,8628.8,1.0,...|  0.0|[5.32554251161007...|[0.99515784990742...|       0.0|\n",
      "|[28.0,8670.98,0.0...|  0.0|[7.59026200904823...|[0.99949490661754...|       0.0|\n",
      "|[28.0,11128.95,1....|  0.0|[4.09749022712612...|[0.98365720318515...|       0.0|\n",
      "|[29.0,5900.78,1.0...|  0.0|[4.06733742325028...|[0.98316533957374...|       0.0|\n",
      "|[29.0,8688.17,1.0...|  1.0|[2.71962101743710...|[0.93817455523842...|       0.0|\n",
      "|[29.0,9378.24,0.0...|  0.0|[4.73007562142384...|[0.99125140974466...|       0.0|\n",
      "|[29.0,12711.15,0....|  0.0|[5.26411648016011...|[0.99485266978467...|       0.0|\n",
      "|[29.0,13240.01,1....|  0.0|[6.47472510820142...|[0.99846045112441...|       0.0|\n",
      "|[29.0,13255.05,1....|  0.0|[4.04666631207394...|[0.98281976763204...|       0.0|\n",
      "|[30.0,7960.64,1.0...|  1.0|[3.13526575434993...|[0.95832420978025...|       0.0|\n",
      "|[30.0,8677.28,1.0...|  0.0|[4.06908153506487...|[0.98319418249941...|       0.0|\n",
      "|[30.0,10744.14,1....|  1.0|[1.77952269189109...|[0.85563791802161...|       0.0|\n",
      "|[30.0,10960.52,1....|  0.0|[2.40247407103320...|[0.91701576933689...|       0.0|\n",
      "|[30.0,11575.37,1....|  1.0|[3.89386120765315...|[0.98003996266427...|       0.0|\n",
      "|[31.0,5387.75,0.0...|  0.0|[2.66975170329194...|[0.93521798988786...|       0.0|\n",
      "|[31.0,8688.21,0.0...|  0.0|[6.48214956423209...|[0.99847182170736...|       0.0|\n",
      "|[31.0,10058.87,1....|  0.0|[4.34147270545395...|[0.98714993040074...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrm_summary.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdoop/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|             churn|         prediction|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|               667|                667|\n",
      "|   mean|0.1634182908545727|0.12293853073463268|\n",
      "| stddev|0.3700243606477147|0.32861306618408714|\n",
      "|    min|               0.0|                0.0|\n",
      "|    max|               1.0|                1.0|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrm_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = lrm.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdoop/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[26.0,8787.39,1.0...|    1|[0.79106248132019...|[0.68805942038441...|       0.0|\n",
      "|[28.0,9090.43,1.0...|    0|[1.61026688857554...|[0.83344843709585...|       0.0|\n",
      "|[28.0,11204.23,0....|    0|[1.97148345642761...|[0.87777036176974...|       0.0|\n",
      "|[28.0,11245.38,0....|    0|[3.75331012854097...|[0.97709682330471...|       0.0|\n",
      "|[29.0,9617.59,0.0...|    0|[4.42202807822146...|[0.98813267415840...|       0.0|\n",
      "|[29.0,10203.18,1....|    0|[3.71080419195966...|[0.97612605863733...|       0.0|\n",
      "|[29.0,11274.46,1....|    0|[4.39058463869247...|[0.98775823667280...|       0.0|\n",
      "|[30.0,6744.87,0.0...|    0|[3.55749267043436...|[0.97228008128416...|       0.0|\n",
      "|[30.0,8403.78,1.0...|    0|[5.76304568829244...|[0.99686830940139...|       0.0|\n",
      "|[30.0,8874.83,0.0...|    0|[3.22709763405855...|[0.96184137137662...|       0.0|\n",
      "|[30.0,10183.98,1....|    0|[2.87524813075597...|[0.94660921217089...|       0.0|\n",
      "|[30.0,12788.37,0....|    0|[2.55636649637209...|[0.92800005897190...|       0.0|\n",
      "|[30.0,13473.35,0....|    0|[2.76977890817311...|[0.94102071697444...|       0.0|\n",
      "|[31.0,5304.6,0.0,...|    0|[3.48758770685488...|[0.97033253112330...|       0.0|\n",
      "|[31.0,7073.61,0.0...|    0|[3.16501389037817...|[0.95949625177380...|       0.0|\n",
      "|[31.0,8829.83,1.0...|    0|[4.33951413571565...|[0.98712506233162...|       0.0|\n",
      "|[31.0,9574.89,0.0...|    0|[3.31877005992135...|[0.96506715076717...|       0.0|\n",
      "|[31.0,11743.24,0....|    0|[6.54958239216506...|[0.99857133106464...|       0.0|\n",
      "|[32.0,6367.22,1.0...|    0|[3.02498369579995...|[0.95369013007749...|       0.0|\n",
      "|[32.0,7896.65,0.0...|    0|[3.45646309561266...|[0.96942330053394...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdoop/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "auc = eval.evaluate(pred_labels.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7456808943089431"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
